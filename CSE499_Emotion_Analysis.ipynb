{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erfan-Mostafiz/CSE499_Emotion-Analysis/blob/Erfan/CSE499_Emotion_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7ykG4kgOu-d",
        "outputId": "002ac314-f154-414c-96dd-4d5dd70568c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LdeqFKH7LAy",
        "outputId": "0c4e1a29-9413-42d7-8a37-ae33eef6eb4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6I7h6Ew9aqb",
        "outputId": "db03e316-f62e-4a59-c904-8665e3dc5552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/CSE499_EmotionAnalysis/DeepVANet-main.zip\n",
            "   creating: DeepVANet-main/\n",
            "  inflating: DeepVANet-main/dataset.py  \n",
            "  inflating: DeepVANet-main/data_preprocess.py  \n",
            "  inflating: DeepVANet-main/decision_level_fusion.py  \n",
            "  inflating: DeepVANet-main/demo.py  \n",
            "  inflating: DeepVANet-main/models.py  \n",
            "  inflating: DeepVANet-main/pretrained_cnn.pth  \n",
            "  inflating: DeepVANet-main/readme.md  \n",
            "  inflating: DeepVANet-main/train.py  \n",
            "  inflating: DeepVANet-main/utils.py  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/gdrive/MyDrive/CSE499_EmotionAnalysis/DeepVANet-main.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Zzsh53okWu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c40dd1cc-e460-4936-d512-a7126e68dcc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYZ2TLWri30V",
        "outputId": "3bd5cd59-7c9b-4da7-9604-62b98e88afd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/gdrive/MyDrive/CSE499_EmotionAnalysis/Reduced/datasets.zip\n",
            "   creating: datasets/\n",
            "   creating: datasets/DEAP/\n",
            "   creating: datasets/DEAP/data_preprocessed_python/\n",
            "   creating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/\n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s01.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s02.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s03.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s04.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s05.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s06.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s07.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s08.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s09.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s10.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s11.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s12.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s13.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s14.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s15.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s16.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s17.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s18.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s19.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s20.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s21.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s22.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s23.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s24.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s25.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s26.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s27.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s28.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s29.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s30.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s31.dat  \n",
            "  inflating: datasets/DEAP/data_preprocessed_python/data_preprocessed_python/s32.dat  \n",
            "  inflating: datasets/DEAP/DEAP.pdf  \n",
            "   creating: datasets/DEAP/face_video/\n",
            "   creating: datasets/DEAP/face_video/s01/\n",
            "  inflating: datasets/DEAP/face_video/s01/s01_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s01/s01_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s01/s01_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s01/s01_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s01/s01_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s01/s01_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s01/s01_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s01/s01_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s01/s01_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s01/s01_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s02/\n",
            "  inflating: datasets/DEAP/face_video/s02/s02_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s02/s02_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s02/s02_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s02/s02_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s02/s02_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s02/s02_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s02/s02_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s02/s02_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s02/s02_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s02/s02_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s03/\n",
            "  inflating: datasets/DEAP/face_video/s03/s03_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s03/s03_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s03/s03_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s03/s03_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s03/s03_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s03/s03_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s03/s03_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s03/s03_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s03/s03_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s03/s03_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s04/\n",
            "  inflating: datasets/DEAP/face_video/s04/s04_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s04/s04_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s04/s04_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s04/s04_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s04/s04_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s04/s04_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s04/s04_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s04/s04_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s04/s04_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s04/s04_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s05/\n",
            "  inflating: datasets/DEAP/face_video/s05/s05_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s05/s05_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s05/s05_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s05/s05_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s05/s05_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s05/s05_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s05/s05_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s05/s05_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s05/s05_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s05/s05_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s06/\n",
            "  inflating: datasets/DEAP/face_video/s06/s06_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s06/s06_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s06/s06_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s06/s06_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s06/s06_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s06/s06_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s06/s06_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s06/s06_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s06/s06_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s06/s06_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s07/\n",
            "  inflating: datasets/DEAP/face_video/s07/s07_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s07/s07_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s07/s07_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s07/s07_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s07/s07_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s07/s07_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s07/s07_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s07/s07_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s07/s07_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s07/s07_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s08/\n",
            "  inflating: datasets/DEAP/face_video/s08/s08_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s08/s08_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s08/s08_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s08/s08_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s08/s08_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s08/s08_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s08/s08_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s08/s08_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s08/s08_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s08/s08_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s09/\n",
            "  inflating: datasets/DEAP/face_video/s09/s09_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s09/s09_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s09/s09_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s09/s09_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s09/s09_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s09/s09_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s09/s09_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s09/s09_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s09/s09_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s09/s09_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s10/\n",
            "  inflating: datasets/DEAP/face_video/s10/s10_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s10/s10_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s10/s10_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s10/s10_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s10/s10_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s10/s10_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s10/s10_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s10/s10_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s10/s10_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s10/s10_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s11/\n",
            "  inflating: datasets/DEAP/face_video/s11/s11_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s11/s11_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s11/s11_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s11/s11_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s11/s11_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s11/s11_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s11/s11_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s11/s11_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s11/s11_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s11/s11_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s12/\n",
            "  inflating: datasets/DEAP/face_video/s12/s12_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s12/s12_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s12/s12_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s12/s12_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s12/s12_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s12/s12_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s12/s12_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s12/s12_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s12/s12_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s12/s12_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s13/\n",
            "  inflating: datasets/DEAP/face_video/s13/s13_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s13/s13_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s13/s13_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s13/s13_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s13/s13_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s13/s13_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s13/s13_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s13/s13_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s13/s13_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s13/s13_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s14/\n",
            "  inflating: datasets/DEAP/face_video/s14/s14_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s14/s14_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s14/s14_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s14/s14_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s14/s14_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s14/s14_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s14/s14_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s14/s14_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s14/s14_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s14/s14_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s15/\n",
            "  inflating: datasets/DEAP/face_video/s15/s15_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s15/s15_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s15/s15_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s15/s15_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s15/s15_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s15/s15_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s15/s15_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s15/s15_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s15/s15_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s15/s15_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s16/\n",
            "  inflating: datasets/DEAP/face_video/s16/s16_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s16/s16_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s16/s16_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s16/s16_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s16/s16_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s16/s16_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s16/s16_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s16/s16_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s16/s16_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s16/s16_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s17/\n",
            "  inflating: datasets/DEAP/face_video/s17/s17_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s17/s17_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s17/s17_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s17/s17_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s17/s17_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s17/s17_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s17/s17_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s17/s17_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s17/s17_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s17/s17_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s18/\n",
            "  inflating: datasets/DEAP/face_video/s18/s18_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s18/s18_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s18/s18_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s18/s18_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s18/s18_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s18/s18_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s18/s18_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s18/s18_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s18/s18_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s18/s18_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s19/\n",
            "  inflating: datasets/DEAP/face_video/s19/s19_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s19/s19_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s19/s19_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s19/s19_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s19/s19_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s19/s19_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s19/s19_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s19/s19_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s19/s19_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s19/s19_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s20/\n",
            "  inflating: datasets/DEAP/face_video/s20/s20_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s20/s20_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s20/s20_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s20/s20_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s20/s20_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s20/s20_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s20/s20_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s20/s20_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s20/s20_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s20/s20_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s21/\n",
            "  inflating: datasets/DEAP/face_video/s21/s21_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s21/s21_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s21/s21_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s21/s21_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s21/s21_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s21/s21_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s21/s21_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s21/s21_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s21/s21_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s21/s21_trial10.avi  \n",
            "   creating: datasets/DEAP/face_video/s22/\n",
            "  inflating: datasets/DEAP/face_video/s22/s22_trial01.avi  \n",
            "  inflating: datasets/DEAP/face_video/s22/s22_trial02.avi  \n",
            "  inflating: datasets/DEAP/face_video/s22/s22_trial03.avi  \n",
            "  inflating: datasets/DEAP/face_video/s22/s22_trial04.avi  \n",
            "  inflating: datasets/DEAP/face_video/s22/s22_trial05.avi  \n",
            "  inflating: datasets/DEAP/face_video/s22/s22_trial06.avi  \n",
            "  inflating: datasets/DEAP/face_video/s22/s22_trial07.avi  \n",
            "  inflating: datasets/DEAP/face_video/s22/s22_trial08.avi  \n",
            "  inflating: datasets/DEAP/face_video/s22/s22_trial09.avi  \n",
            "  inflating: datasets/DEAP/face_video/s22/s22_trial10.avi  \n",
            "   creating: datasets/DEAP/metadata_csv/\n",
            "  inflating: datasets/DEAP/metadata_csv/online_ratings.csv  \n",
            "  inflating: datasets/DEAP/metadata_csv/participant_questionnaire.csv  \n",
            "  inflating: datasets/DEAP/metadata_csv/participant_ratings.csv  \n",
            "  inflating: datasets/DEAP/metadata_csv/video_list.csv  \n",
            "   creating: datasets/DEAP/metadata_xls/\n",
            "  inflating: datasets/DEAP/metadata_xls/online_ratings.xls  \n",
            "  inflating: datasets/DEAP/metadata_xls/participant_questionnaire.xls  \n",
            "  inflating: datasets/DEAP/metadata_xls/participant_ratings.xls  \n",
            "  inflating: datasets/DEAP/metadata_xls/video_list.xls  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/gdrive/MyDrive/CSE499_EmotionAnalysis/Reduced/datasets.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oJHSerJ7r1e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision import transforms as T\n",
        "import io\n",
        "import zipfile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC08mRAatBvx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aVQJdbmfc0r"
      },
      "outputs": [],
      "source": [
        "# !unzip \"/content/gdrive/Shareddrives/DEAP dataset/face_video.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56AAq8Em0oLz"
      },
      "outputs": [],
      "source": [
        "class DEAP(data.Dataset):\n",
        "    '''\n",
        "    DEAP dataset for per-subject experiments\n",
        "    Parameters:\n",
        "        modal   : data modality; {'face', 'eeg', 'peri', 'bio', 'faceeeg', 'faceperi', 'facebio'}, default = 'facebio'.\n",
        "        subject : subject ID; an integer between 1 and 22, default = 1.\n",
        "        k       : kth fold; an integer between 1 and 10, default = 1.\n",
        "        kind    : dataset type; {'train', 'val', 'all'}, default = 'all'.\n",
        "        indices : index of data samples (for dataset shuffle); an list of integers, default = list(range(2400)).\n",
        "        label   : emotion label; {'valence', 'arousal'}, defualt = 'valence'.\n",
        "    '''\n",
        "    def __init__(self, modal='facebio', subject=1, k=1, kind='all', indices=list(range(2400)),label='valence'):\n",
        "        self.modal = modal\n",
        "        self.subject = subject\n",
        "        self.k = k\n",
        "        self.kind = kind\n",
        "        self.label = label\n",
        "        self.bio_path = f'./data/DEAP/bio/s{subject}.zip'\n",
        "        self.label_path = f'./data/DEAP/labels/'\n",
        "        self.face_path = f'./data/DEAP/faces/s{subject}.zip'\n",
        "        self.labels = pd.read_csv(self.label_path+'participant_ratings.csv')\n",
        "        self.face_zip = zipfile.ZipFile(self.face_path, 'r')\n",
        "        self.bio_zip = zipfile.ZipFile(self.bio_path, 'r')\n",
        "        self.size = len(indices)\n",
        "\n",
        "        if kind == 'train':\n",
        "            self.indices = indices[:int((k - 1) * self.size / 10)] + indices[int(k * self.size / 10):]\n",
        "        if kind == 'val':\n",
        "            self.indices = indices[int((k - 1) * self.size / 10):int(k * self.size / 10)]\n",
        "        if kind == 'all':\n",
        "            self.indices = indices\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        index = self.indices[i]\n",
        "        trial = index // 60 + 1\n",
        "        segment = index % 60 + 1\n",
        "        prex = 's' + (str(self.subject) if self.subject > 9 else '0' + str(self.subject)) + '/s' + (\n",
        "            str(self.subject) if self.subject > 9 else '0' + str(self.subject)) + '_trial' + (\n",
        "                   str(trial) if trial > 9 else '0' + str(trial)) + '/s' + (\n",
        "                   str(self.subject) if self.subject > 9 else '0' + str(self.subject)) + '_trial' + (\n",
        "                   str(trial) if trial > 9 else '0' + str(trial))\n",
        "        transform = T.Compose([T.Resize((64, 64)),\n",
        "                               T.ToTensor()])\n",
        "\n",
        "        face_data = []\n",
        "        for n in range(1, 6):\n",
        "            img = Image.open(io.BytesIO(self.face_zip.read(prex + f'_{(segment - 1) * 5 + n}.png')))\n",
        "            frame_array = transform(img)\n",
        "            frame_array = frame_array.view(1, 3, 64, 64)\n",
        "            face_data.append(frame_array)\n",
        "        face_data = torch.cat(face_data, dim=0)\n",
        "        bio_data = torch.tensor(\n",
        "            np.load(io.BytesIO(self.bio_zip.read(f's{self.subject}/{self.subject}_{trial}_{segment}.npy')))).float()\n",
        "\n",
        "        if self.modal == 'face':\n",
        "            data = face_data\n",
        "        elif self.modal == 'eeg':\n",
        "            data = bio_data[:32]\n",
        "        elif self.modal == 'peri':\n",
        "            data = bio_data[32:]\n",
        "        elif self.modal == 'bio':\n",
        "            data = bio_data\n",
        "        elif self.modal == 'faceeeg':\n",
        "            data = (face_data, bio_data[:32])\n",
        "        elif self.modal == 'faceperi':\n",
        "            data = (face_data, bio_data[32:])\n",
        "        elif self.modal == 'facebio':\n",
        "            data = (face_data, bio_data)\n",
        "\n",
        "        valence = 0 if self.labels[(self.labels['Participant_id']==self.subject) & (self.labels['Trial']==trial)]['Valence'].iloc[0] < 5 else 1\n",
        "        arousal = 0 if self.labels[(self.labels['Participant_id']==self.subject) & (self.labels['Trial']==trial)]['Arousal'].iloc[0] < 5 else 1\n",
        "        if self.label == 'valence':\n",
        "            return data, valence\n",
        "        else:\n",
        "            return data, arousal\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW2hME810_DB"
      },
      "outputs": [],
      "source": [
        "class DEAPAll(data.Dataset):\n",
        "    def __init__(self, modal='facebio', k=1, kind='all', indices=list(range(52440)),label='valence'):\n",
        "        self.modal = modal\n",
        "        self.k = k\n",
        "        self.kind = kind\n",
        "        self.label = label\n",
        "        self.bio_path = f'./data/DEAP/bio/'\n",
        "        self.label_path = f'./data/DEAP/labels/'\n",
        "        self.face_path = f'./data/DEAP/faces/'\n",
        "        self.labels = pd.read_csv(self.label_path+'participant_ratings.csv')\n",
        "        deap_indices_dict = {1: 2400,\n",
        "                             2: 2400,\n",
        "                             3: 2340,\n",
        "                             4: 2400,\n",
        "                             5: 2340,\n",
        "                             6: 2400,\n",
        "                             7: 2400,\n",
        "                             8: 2400,\n",
        "                             9: 2400,\n",
        "                             10: 2400,\n",
        "                             11: 2220,\n",
        "                             12: 2400,\n",
        "                             13: 2400,\n",
        "                             14: 2340,\n",
        "                             15: 2400,\n",
        "                             16: 2400,\n",
        "                             17: 2400,\n",
        "                             18: 2400,\n",
        "                             19: 2400,\n",
        "                             20: 2400,\n",
        "                             21: 2400,\n",
        "                             22: 2400}\n",
        "        self.sub_trial_seg = []\n",
        "        for sub in range(1,23):\n",
        "            for trial in range(1,int(deap_indices_dict[sub]/60+1)):\n",
        "                for seg in range(1,61):\n",
        "                    self.sub_trial_seg.append((sub,trial,seg))\n",
        "        self.size = len(indices)\n",
        "\n",
        "        if kind == 'train':\n",
        "            self.indices = indices[:int((k - 1) * self.size / 10)] + indices[int(k * self.size / 10):]\n",
        "        if kind == 'val':\n",
        "            self.indices = indices[int((k - 1) * self.size / 10):int(k * self.size / 10)]\n",
        "        if kind == 'all':\n",
        "            self.indices = indices\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        index = self.indices[i]\n",
        "        subject, trial, segment = self.sub_trial_seg[index]\n",
        "        face_zip = zipfile.ZipFile(self.face_path+f's{subject}.zip', 'r')\n",
        "        bio_zip = zipfile.ZipFile(self.bio_path+f's{subject}.zip', 'r')\n",
        "        prex = 's' + (str(subject) if subject > 9 else '0' + str(subject)) + '/s' + (\n",
        "            str(subject) if subject > 9 else '0' + str(subject)) + '_trial' + (\n",
        "                   str(trial) if trial > 9 else '0' + str(trial)) + '/s' + (\n",
        "                   str(subject) if subject > 9 else '0' + str(subject)) + '_trial' + (\n",
        "                   str(trial) if trial > 9 else '0' + str(trial))\n",
        "        transform = T.Compose([T.Resize((64, 64)),\n",
        "                               T.ToTensor()])\n",
        "        face_data = []\n",
        "        for n in range(1, 6):\n",
        "            img = Image.open(io.BytesIO(face_zip.read(prex + f'_{(segment - 1) * 5 + n}.png')))\n",
        "            frame_array = transform(img)\n",
        "            frame_array = frame_array.view(1, 3, 64, 64)\n",
        "            face_data.append(frame_array)\n",
        "        face_data = torch.cat(face_data, dim=0)\n",
        "\n",
        "        bio_data = torch.tensor(np.load(io.BytesIO(bio_zip.read(f's{subject}/{subject}_{trial}_{segment}.npy')))).float()\n",
        "\n",
        "        if self.modal == 'face':\n",
        "            data = face_data\n",
        "        elif self.modal == 'eeg':\n",
        "            data = bio_data[:32]\n",
        "        elif self.modal == 'peri':\n",
        "            data = bio_data[32:]\n",
        "        elif self.modal == 'bio':\n",
        "            data = bio_data\n",
        "        elif self.modal == 'faceeeg':\n",
        "            data = (face_data, bio_data[:32])\n",
        "        elif self.modal == 'faceperi':\n",
        "            data = (face_data, bio_data[32:])\n",
        "        elif self.modal == 'facebio':\n",
        "            data = (face_data, bio_data)\n",
        "\n",
        "        valence = 0 if self.labels[(self.labels['Participant_id']==subject) & (self.labels['Trial']==trial)]['Valence'].iloc[0] < 5 else 1\n",
        "        arousal = 0 if self.labels[(self.labels['Participant_id']==subject) & (self.labels['Trial']==trial)]['Arousal'].iloc[0] < 5 else 1\n",
        "        if self.label == 'valence':\n",
        "            return data, valence\n",
        "        else:\n",
        "            return data, arousal\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKXZMg3c29o1",
        "outputId": "144c4cf9-5567-4977-b478-db4ae18542d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting face_alignment\n",
            "  Downloading face_alignment-1.3.5.tar.gz (27 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from face_alignment) (1.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_alignment) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from face_alignment) (1.7.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from face_alignment) (0.18.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from face_alignment) (4.1.2.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from face_alignment) (4.64.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from face_alignment) (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->face_alignment) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->face_alignment) (57.4.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (2.6.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (7.1.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (1.3.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->face_alignment) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->face_alignment) (1.15.0)\n",
            "Building wheels for collected packages: face-alignment\n",
            "  Building wheel for face-alignment (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-alignment: filename=face_alignment-1.3.5-py2.py3-none-any.whl size=28241 sha256=c0473e68eebb5e4d8474080039aa9a5d2a356a82a35784a661f08406b56c86c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/ba/4d/2d368f55e5f929f9472da59e356fbdf1483f885de80a5bc620\n",
            "Successfully built face-alignment\n",
            "Installing collected packages: face-alignment\n",
            "Successfully installed face-alignment-1.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install face_alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gz3yl1Ly3Oja",
        "outputId": "64a26a07-5c0c-4db2-96e5-e195fe8a2ce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyedflib\n",
            "  Downloading pyEDFlib-0.1.30-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from pyedflib) (1.21.6)\n",
            "Installing collected packages: pyedflib\n",
            "Successfully installed pyedflib-0.1.30\n"
          ]
        }
      ],
      "source": [
        "!pip install pyedflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYXggntH1H3I"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Functions for data pre-process\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "import pickle as cPickle\n",
        "import os\n",
        "import face_alignment\n",
        "from xml.dom.minidom import parse\n",
        "import pyedflib\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0vJ4Vgf2W63"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ************************* Face Data Pre-process *************************\n",
        "\n",
        "def video2frames(dataset='DEAP'):\n",
        "    '''\n",
        "    Extract frames from videos.\n",
        "    :param dataset: used dataset\n",
        "    '''\n",
        "    # assert dataset in ['DEAP', 'MAHNOB'], 'Invalid dataset name'\n",
        "    assert dataset in ['DEAP'], 'Invalid dataset name'\n",
        "\n",
        "    if dataset == 'DEAP':\n",
        "        dataset_path = './datasets/DEAP/face_video/'\n",
        "        des_path = './datasets/DEAP/frames/'\n",
        "        for subject in os.listdir(dataset_path):\n",
        "            if subject.startswith('.'):\n",
        "                continue\n",
        "            sub_path = dataset_path+subject\n",
        "            for video_file in os.listdir(sub_path):\n",
        "                if not os.path.exists(des_path + subject):\n",
        "                    os.mkdir(des_path + subject)\n",
        "                if not os.path.exists(des_path + subject + '/' + video_file.split('.')[0]):\n",
        "                    os.mkdir(des_path + subject + '/' + video_file.split('.')[0])\n",
        "                video_file_path = sub_path+'/'+video_file\n",
        "                video = cv2.VideoCapture(video_file_path)\n",
        "                c = 1\n",
        "                frame_rate = 10\n",
        "                count = 0\n",
        "                while (True):\n",
        "                    ret, frame = video.read()\n",
        "                    if ret:\n",
        "                        if (c % frame_rate == 0):\n",
        "                            count += 1\n",
        "                            cv2.imwrite(des_path+subject+'/'+video_file.split('.')[0] +'/'+ video_file.split('.')[0]+'_'+str(count) + '.png', frame)\n",
        "                        c += 1\n",
        "                        cv2.waitKey(0)\n",
        "                    else:\n",
        "                        break\n",
        "                video.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3aQlX-Z3fjO"
      },
      "outputs": [],
      "source": [
        "# functions for face alignment and cropping are based on https://github.com/DANNALI35/zhihu_article/tree/master/201901_face_alignment\n",
        "def to_dict(landmarks):\n",
        "    '''\n",
        "    Transfer detected facial landmarks list to dictionary.\n",
        "    :param landmarks: a list of facial landmarks\n",
        "    :return: a dictionary of facial landmarks\n",
        "    '''\n",
        "    l = list()\n",
        "    for i in range(68):\n",
        "        point = (landmarks[i][0], landmarks[i][1])\n",
        "        l.append(point)\n",
        "    face_landmarks_dict = dict()\n",
        "    face_landmarks_dict['chin'] = l[0:17]\n",
        "    face_landmarks_dict['left_eyebrow'] = l[17:22]\n",
        "    face_landmarks_dict['right_eyebrow'] = l[22:27]\n",
        "    face_landmarks_dict['nose_bridge'] = l[27:31]\n",
        "    face_landmarks_dict['nose_tip'] = l[31:36]\n",
        "    face_landmarks_dict['left_eye'] = l[36:42]\n",
        "    face_landmarks_dict['right_eye'] = l[42:48]\n",
        "    face_landmarks_dict['top_lip'] = l[48:55] + l[60:65]\n",
        "    face_landmarks_dict['bottom_lip'] = l[55:60] + l[65:68]\n",
        "    return face_landmarks_dict\n",
        "\n",
        "\n",
        "def crop_face(image_array, landmarks):\n",
        "    \"\"\" crop face according to eye,mouth and chin position\n",
        "    :param image_array: numpy array of a single image\n",
        "    :param landmarks: dict of landmarks for facial parts as keys and tuple of coordinates as values\n",
        "    :return:\n",
        "    cropped_img: numpy array of cropped image\n",
        "    \"\"\"\n",
        "\n",
        "    eye_landmark = np.concatenate([np.array(landmarks['left_eye']),\n",
        "                                   np.array(landmarks['right_eye'])])\n",
        "    eye_center = np.mean(eye_landmark, axis=0).astype(\"int\")\n",
        "    lip_landmark = np.concatenate([np.array(landmarks['top_lip']),\n",
        "                                   np.array(landmarks['bottom_lip'])])\n",
        "    lip_center = np.mean(lip_landmark, axis=0).astype(\"int\")\n",
        "    mid_part = lip_center[1] - eye_center[1]\n",
        "    top = eye_center[1] - mid_part * 18 / 40\n",
        "    bottom = lip_center[1] + mid_part * 12 / 40\n",
        "\n",
        "    w = h = bottom - top\n",
        "    x_center = eye_center[0]\n",
        "    left, right = (x_center - w / 2, x_center + w / 2)\n",
        "\n",
        "    pil_img = Image.fromarray(image_array)\n",
        "    left, top, right, bottom = [int(i) for i in [left, top, right, bottom]]\n",
        "    cropped_img = pil_img.crop((left, top, right, bottom))\n",
        "    cropped_img = np.array(cropped_img)\n",
        "    return cropped_img, left, top"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBeVvprv3mYC"
      },
      "outputs": [],
      "source": [
        "def rotate_landmarks(landmarks, eye_center, angle, row):\n",
        "    \"\"\" rotate landmarks to fit the aligned face\n",
        "    :param landmarks: dict of landmarks for facial parts as keys and tuple of coordinates as values\n",
        "    :param eye_center: tuple of coordinates for eye center\n",
        "    :param angle: degrees of rotation\n",
        "    :param row: row size of the image\n",
        "    :return: rotated_landmarks with the same structure with landmarks, but different values\n",
        "    \"\"\"\n",
        "    rotated_landmarks = defaultdict(list)\n",
        "    for facial_feature in landmarks.keys():\n",
        "        for landmark in landmarks[facial_feature]:\n",
        "            rotated_landmark = rotate(origin=eye_center, point=landmark, angle=angle, row=row)\n",
        "            rotated_landmarks[facial_feature].append(rotated_landmark)\n",
        "    return rotated_landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SvgI4mY3p9X"
      },
      "outputs": [],
      "source": [
        "def rotate(origin, point, angle, row):\n",
        "    \"\"\" rotate coordinates in image coordinate system\n",
        "    :param origin: tuple of coordinates,the rotation center\n",
        "    :param point: tuple of coordinates, points to rotate\n",
        "    :param angle: degrees of rotation\n",
        "    :param row: row size of the image\n",
        "    :return: rotated coordinates of point\n",
        "    \"\"\"\n",
        "    x1, y1 = point\n",
        "    x2, y2 = origin\n",
        "    y1 = row - y1\n",
        "    y2 = row - y2\n",
        "    angle = math.radians(angle)\n",
        "    x = x2 + math.cos(angle) * (x1 - x2) - math.sin(angle) * (y1 - y2)\n",
        "    y = y2 + math.sin(angle) * (x1 - x2) + math.cos(angle) * (y1 - y2)\n",
        "    y = row - y\n",
        "    return int(x), int(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KsqrlsP3vLm"
      },
      "outputs": [],
      "source": [
        "def align_face(image_array, landmarks):\n",
        "    \"\"\" align faces according to eyes position\n",
        "    :param image_array: numpy array of a single image\n",
        "    :param landmarks: dict of landmarks for facial parts as keys and tuple of coordinates as values\n",
        "    :return:\n",
        "    rotated_img:  numpy array of aligned image\n",
        "    eye_center: tuple of coordinates for eye center\n",
        "    angle: degrees of rotation\n",
        "    \"\"\"\n",
        "    # get list landmarks of left and right eye\n",
        "    left_eye = landmarks['left_eye']\n",
        "    right_eye = landmarks['right_eye']\n",
        "    # calculate the mean point of landmarks of left and right eye\n",
        "    left_eye_center = np.mean(left_eye, axis=0).astype(\"int\")\n",
        "    right_eye_center = np.mean(right_eye, axis=0).astype(\"int\")\n",
        "    # compute the angle between the eye centroids\n",
        "    dy = right_eye_center[1] - left_eye_center[1]\n",
        "    dx = right_eye_center[0] - left_eye_center[0]\n",
        "    # compute angle between the line of 2 centeroids and the horizontal line\n",
        "    angle = math.atan2(dy, dx) * 180. / math.pi\n",
        "    # calculate the center of 2 eyes\n",
        "    eye_center = ((left_eye_center[0] + right_eye_center[0]) // 2,\n",
        "                  (left_eye_center[1] + right_eye_center[1]) // 2)\n",
        "    # at the eye_center, rotate the image by the angle\n",
        "    rotate_matrix = cv2.getRotationMatrix2D(eye_center, angle, scale=1)\n",
        "    rotated_img = cv2.warpAffine(image_array, rotate_matrix, (image_array.shape[1], image_array.shape[0]))\n",
        "    return rotated_img, eye_center, angle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_RGaarq32uM"
      },
      "outputs": [],
      "source": [
        "def align_landmarks(landmarks):\n",
        "    left_eye = landmarks['left_eye']\n",
        "    right_eye = landmarks['right_eye']\n",
        "    # calculate the mean point of landmarks of left and right eye\n",
        "    left_eye_center = np.mean(left_eye, axis=0).astype(\"int\")\n",
        "    right_eye_center = np.mean(right_eye, axis=0).astype(\"int\")\n",
        "    # compute the angle between the eye centroids\n",
        "    dy = right_eye_center[1] - left_eye_center[1]\n",
        "    dx = right_eye_center[0] - left_eye_center[0]\n",
        "    # compute angle between the line of 2 centeroids and the horizontal line\n",
        "    angle = math.atan2(dy, dx) * 180. / math.pi\n",
        "    # calculate the center of 2 eyes\n",
        "    eye_center = ((left_eye_center[0] + right_eye_center[0]) // 2,\n",
        "                  (left_eye_center[1] + right_eye_center[1]) // 2)\n",
        "#     rotated_landmarks = defaultdict(list)\n",
        "    rotated_landmarks = []\n",
        "    for facial_feature in landmarks.keys():\n",
        "        for landmark in landmarks[facial_feature]:\n",
        "            rotated_landmark = rotate(origin=eye_center, point=landmark, angle=angle, row=570)\n",
        "#             rotated_landmarks[facial_feature].append(rotated_landmark)\n",
        "            rotated_landmarks.append(rotated_landmark)\n",
        "    return rotated_landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzUMtL6E3_lu"
      },
      "outputs": [],
      "source": [
        "def face_detection_alignment_cropping(dataset='DEAP'):\n",
        "    '''\n",
        "    Transfer frames to faces by face detection, alignment and cropping.\n",
        "    :param dataset: used dataset\n",
        "    '''\n",
        "    # assert dataset in ['DEAP', 'MAHNOB'], 'Invalid dataset name'\n",
        "    assert dataset in ['DEAP'], 'Invalid dataset name'\n",
        "\n",
        "    # facial landmarks detector; use gpu by changing device parameter to 'cuda', or do not use gpu by changing device parameter to 'cpu'\n",
        "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False, device='cpu')\n",
        "\n",
        "    if dataset == 'DEAP':\n",
        "        root = './datasets/DEAP/frames/'\n",
        "        des_path = './data/DEAP/faces/'\n",
        "\n",
        "    # if dataset == 'MAHNOB':\n",
        "    #     root = './datasets/MAHNOB/frames/'\n",
        "    #     des_path = './data/DEAP/faces/'\n",
        "\n",
        "    for subject in os.listdir(root):\n",
        "        for trial in os.listdir(root+subject):\n",
        "            # if os.path.exists(des_path + subject + '/' + trial):\n",
        "            #     continue\n",
        "            os.mkdir(des_path + subject + '/' + trial)\n",
        "            if os.path.exists(des_path + subject + '/' + trial):\n",
        "                continue\n",
        "            for frame in os.listdir(root+subject+'/'+trial):\n",
        "                frame_path = root + subject + '/' + trial + '/' + frame\n",
        "                img = cv2.imread(frame_path)\n",
        "                preds = fa.get_landmarks(img)\n",
        "                try:\n",
        "                    landmarks_list = preds[0]\n",
        "                    landmarks_dict = to_dict(landmarks_list)\n",
        "                    aligned_face, eye_center, angle = align_face(image_array=img, landmarks=landmarks_dict)\n",
        "                    rotated_landmarks = rotate_landmarks(landmarks=landmarks_dict, eye_center=eye_center, angle=angle,\n",
        "                                                     row=img.shape[0])\n",
        "                    cropped_img, left, top = crop_face(image_array=aligned_face, landmarks=rotated_landmarks)\n",
        "\n",
        "                    cv2.imwrite(des_path + subject + '/' + trial + '/' + frame, cropped_img)\n",
        "                except:\n",
        "                    print(f'Fail to get the face image: {frame}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88s_dr4_4Iqh"
      },
      "outputs": [],
      "source": [
        "# ************************* Bio-sensing Data Pre-process *************************\n",
        "\n",
        "def trial2segments(dataset='DEAP'):\n",
        "    '''\n",
        "    Divide bio-sensing data of each trial to 1-second length segments, and perform baseline removal.\n",
        "    Note, when dealing with MAHNOB-HCI dataset, EEG data should be common reference averaged, bandpass filtered and artefact removed using EEGLab,\n",
        "    and preprocessed EEG data files (one file per trial) should be stored in './datasets/MAHNOB/eeg_preprocessed/ folder in .npy format.\n",
        "    :param dataset: used dataset\n",
        "    '''\n",
        "    # assert dataset in ['DEAP', 'MAHNOB'], 'Invalid dataset name'\n",
        "    assert dataset in ['DEAP'], 'Invalid dataset name'\n",
        "\n",
        "    if dataset == 'DEAP':\n",
        "        root = './datasets/DEAP/data_preprocessed_python/'\n",
        "        des_path = './data/DEAP/bio/'\n",
        "        labels = pd.read_csv('./data/DEAP/labels/participant_ratings.csv')\n",
        "        for file in os.listdir(root):\n",
        "            subject = file.split('.')[0]\n",
        "            sub_id = int(subject[1:])\n",
        "            os.mkdir(des_path + 's' + str(sub_id))\n",
        "            f = open(root + file, 'rb')\n",
        "            d = cPickle.load(f, encoding='latin1')\n",
        "            data = d['data']\n",
        "            for experiment in range(40):\n",
        "                trial = labels[(labels['Participant_id'] == sub_id) & (labels['Experiment_id'] == experiment + 1)][\n",
        "                    'Trial'].iloc[0]\n",
        "                # baseline\n",
        "                l = []\n",
        "                for i in range(3):\n",
        "                    l.append(data[experiment][:, i * 128:(i + 1) * 128])\n",
        "                baseline_mean = sum(l) / 3\n",
        "                # segments\n",
        "                for i in range(60):\n",
        "                    data_seg = data[experiment][:, 384 + i * 128:384 + (i + 1) * 128]\n",
        "                    data_seg_removed = data_seg - baseline_mean\n",
        "                    np.save(f'{des_path}s{sub_id}/{sub_id}_{trial}_{i + 1}.npy', data_seg_removed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtAgPZ0M4OLn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "a0f2bc1d-4cc4-4684-9b30-fe3408d37e7a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-f4434eaa3f23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mpreprocess_demo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-67-f4434eaa3f23>\u001b[0m in \u001b[0;36mpreprocess_demo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./datasets/DEAP/metadata_csv/participant_ratings.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./data/DEAP/labels/participant_ratings.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mvideo2frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DEAP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mface_detection_alignment_cropping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DEAP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# preprocess bio-sensing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-0b456312053d>\u001b[0m in \u001b[0;36mface_detection_alignment_cropping\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# if os.path.exists(des_path + subject + '/' + trial):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m#     continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdes_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdes_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/DEAP/faces/s04/s04_trial06'"
          ]
        }
      ],
      "source": [
        "def preprocess_demo():\n",
        "    '''\n",
        "    This function pre-processes DEAP dataset.\n",
        "    Please unzip face_video.zip, data_preprocessed_python.zip and metadata_csv.zip from DEAP dataset in './datasets/DEAP/'.\n",
        "    Then call this function, the preprocessed data will be stored in './data/DEAP/'.\n",
        "    It is recommended to use a device with GPU, otherwise the face detection will be slow.\n",
        "    Note that faces cannot be detected from some frames, these frames should be replaced with the neighbour frame manually.\n",
        "    '''\n",
        "    # pre-process face data\n",
        "    if not os.path.exists('./datasets/DEAP/frames/'):\n",
        "        os.mkdir('./datasets/DEAP/frames/')\n",
        "    if not os.path.exists('./data/'):\n",
        "        os.mkdir('./data/')\n",
        "    if not os.path.exists('./data/DEAP/'):\n",
        "        os.mkdir('./data/DEAP/')\n",
        "    if not os.path.exists('./data/DEAP/faces/'):\n",
        "        os.mkdir('./data/DEAP/faces/')\n",
        "    if not os.path.exists('./data/DEAP/labels/'):\n",
        "        os.mkdir('./data/DEAP/labels/')\n",
        "    shutil.copy('./datasets/DEAP/metadata_csv/participant_ratings.csv', './data/DEAP/labels/participant_ratings.csv')\n",
        "    video2frames('DEAP')\n",
        "    face_detection_alignment_cropping('DEAP')\n",
        "\n",
        "    # preprocess bio-sensing data\n",
        "    if not os.path.exists('./data/DEAP/bio/'):\n",
        "        os.mkdir('./data/DEAP/bio/')\n",
        "    trial2segments('DEAP')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    preprocess_demo()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural network models in DeepVANet"
      ],
      "metadata": {
        "id": "lrscB8Umhhsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time"
      ],
      "metadata": {
        "id": "ItcJyVfMhndz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The implementation of CONVLSTM are based on the code from\n",
        "# https://github.com/ndrplz/ConvLSTM_pytorch/blob/master/convlstm.py\n",
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, padding):\n",
        "        \"\"\"\n",
        "        Initialize ConvLSTM cell.\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size: int\n",
        "            Size of the convolutional kernel.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim,\n",
        "                              kernel_size=kernel_size,\n",
        "                              padding=padding)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
        "\n",
        "        combined_conv = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, height, width):\n",
        "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
        "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))"
      ],
      "metadata": {
        "id": "iWkFjL3NhvKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size=3, padding=1):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.cell = ConvLSTMCell(input_dim=self.input_dim,\n",
        "                                 hidden_dim=self.hidden_dim,kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "    def forward(self, input_tensor, time=None):\n",
        "\n",
        "        b, _, _, h, w = input_tensor.size()\n",
        "\n",
        "        hidden_state = self.cell.init_hidden(b,h,w)\n",
        "\n",
        "        seq_len = input_tensor.size(1)\n",
        "\n",
        "        h, c = hidden_state\n",
        "        for t in range(seq_len):\n",
        "            h, c = self.cell(input_tensor=input_tensor[:, t, :, :, :], cur_state=[h, c])\n",
        "        return h"
      ],
      "metadata": {
        "id": "e1XH3yiwhxN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceFeatureExtractorCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FaceFeatureExtractorCNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128 * 6 * 6, 1000),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1000, 2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.net(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def save(self, name=None):\n",
        "        \"\"\"\n",
        "        save the model\n",
        "        \"\"\"\n",
        "        if name is None:\n",
        "            prefix = 'FaceFeatureExtractorCNN_'\n",
        "            name = time.strftime(prefix + '%m%d_%H:%M:%S.pth')\n",
        "        torch.save(self.state_dict(), name)\n",
        "        return name\n",
        "\n",
        "    def load(self, path):\n",
        "        # self.load_state_dict(torch.load(path))\n",
        "        self.load_state_dict(torch.load(path,map_location=torch.device('cpu')))\n"
      ],
      "metadata": {
        "id": "KbI5nM8KiJGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceFeatureExtractor(nn.Module):\n",
        "    def __init__(self, feature_size=16, pretrain=True):\n",
        "        super(FaceFeatureExtractor, self).__init__()\n",
        "        cnn = FaceFeatureExtractorCNN()\n",
        "        if pretrain:\n",
        "            cnn.load('./pretrained_cnn.pth')\n",
        "        self.cnn = cnn.net\n",
        "        self.rnn = ConvLSTM(128, 128)\n",
        "        self.fc = nn.Linear(128*6*6, feature_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input should be 5 dimension: (B, T, C, H, W)\n",
        "        b, t, c, h, w = x.shape\n",
        "        x = x.view(b * t, c, h, w)\n",
        "        cnn_output = self.cnn(x)\n",
        "        rnn_input = cnn_output.view(b, t, 128, 6, 6)\n",
        "        rnn_output = self.rnn(rnn_input)\n",
        "        rnn_output = torch.flatten(rnn_output, 1)\n",
        "        output = self.fc(rnn_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "a9xEvSEuiLxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BioFeatureExtractor(nn.Module):\n",
        "    def __init__(self, input_size=32, feature_size=40):\n",
        "        super(BioFeatureExtractor, self).__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=input_size, out_channels=24, kernel_size=5),\n",
        "            nn.BatchNorm1d(num_features=24),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(in_channels=24, out_channels=16, kernel_size=3),\n",
        "            nn.BatchNorm1d(num_features=16),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv1d(in_channels=16, out_channels=8, kernel_size=3),\n",
        "            nn.BatchNorm1d(num_features=8),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fc = nn.Linear(8*120, feature_size)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.cnn(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "A9LnXKJ6iMGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepVANetVision(nn.Module):\n",
        "    def __init__(self,feature_size=16,pretrain=True):\n",
        "        super(DeepVANetVision,self).__init__()\n",
        "        self.features = FaceFeatureExtractor(feature_size=feature_size,pretrain=pretrain)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(feature_size, 20),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(20, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.squeeze(-1)\n",
        "        return x\n",
        "\n",
        "    def save(self, name=None):\n",
        "        \"\"\"\n",
        "        save the model\n",
        "        \"\"\"\n",
        "        if name is None:\n",
        "            prefix = 'checkpoints/' + 'face_classifier_'\n",
        "            name = time.strftime(prefix + '%m%d_%H:%M:%S.pth')\n",
        "        torch.save(self.state_dict(), name)\n",
        "        return name\n",
        "\n",
        "    def load(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "        # self.load_state_dict(torch.load(path,map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "CIoHOly0iMUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepVANetBio(nn.Module):\n",
        "    def __init__(self, input_size=32, feature_size=64):\n",
        "        super(DeepVANetBio, self).__init__()\n",
        "        self.features = BioFeatureExtractor(input_size=input_size, feature_size=feature_size)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(feature_size, 20),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(20, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = x.squeeze(-1)\n",
        "        return x\n",
        "\n",
        "    def save(self, name=None):\n",
        "        \"\"\"\n",
        "        save the model\n",
        "        \"\"\"\n",
        "        if name is None:\n",
        "            prefix = 'checkpoints/' + 'physiological_classifier_'\n",
        "            name = time.strftime(prefix + '%m%d_%H:%M:%S.pth')\n",
        "        torch.save(self.state_dict(), name)\n",
        "        return name\n",
        "\n",
        "    def load(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "        # self.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n"
      ],
      "metadata": {
        "id": "u5mUSw4QiTdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepVANet(nn.Module):\n",
        "    def __init__(self, bio_input_size=32, face_feature_size=16, bio_feature_size=64,pretrain=True):\n",
        "        super(DeepVANet,self).__init__()\n",
        "        self.face_feature_extractor = FaceFeatureExtractor(feature_size=face_feature_size,pretrain=pretrain)\n",
        "\n",
        "        self.bio_feature_extractor = BioFeatureExtractor(input_size=bio_input_size, feature_size=bio_feature_size)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(face_feature_size + bio_feature_size, 20),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(20, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        img_features = self.face_feature_extractor(x[0])\n",
        "        bio_features = self.bio_feature_extractor(x[1])\n",
        "        features = torch.cat([img_features,bio_features.float()],dim=1)\n",
        "        output = self.classifier(features)\n",
        "        output = output.squeeze(-1)\n",
        "        return output\n",
        "\n",
        "    def save(self, name=None):\n",
        "        \"\"\"\n",
        "        save the model\n",
        "        \"\"\"\n",
        "        if name is None:\n",
        "            prefix = 'checkpoints/' + 'fusion_classifier_'\n",
        "            name = time.strftime(prefix + '%m%d_%H:%M:%S.pth')\n",
        "        torch.save(self.state_dict(), name)\n",
        "        return name\n",
        "\n",
        "    def load(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "        # self.load_state_dict(torch.load(path,map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "UnCz-Tu_iW9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Util functions"
      ],
      "metadata": {
        "id": "5CO8wkunjr0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def out_put(string, verbose):\n",
        "    '''\n",
        "    Help function for verbose,\n",
        "    output the string to destination path\n",
        "    Parameters\n",
        "    ----------\n",
        "    string  :str,  the string to output\n",
        "    verbose :str, the path to store the output\n",
        "    '''\n",
        "    with open(f\"{verbose}.txt\", \"a\") as f:\n",
        "        f.write(string + \"\\n\")"
      ],
      "metadata": {
        "id": "VOwLJV3Xjt6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained_cnn.pth"
      ],
      "metadata": {
        "id": "12RTcPQAmQA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = torch.load(\"/content/DeepVANet-main/pretrained_cnn.pth\")\n",
        "model = torch.load(\"/content/DeepVANet-main/pretrained_cnn.pth\",map_location ='cpu')"
      ],
      "metadata": {
        "id": "Hi_Rac_9mXDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and test"
      ],
      "metadata": {
        "id": "raZb71nOjhMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSZdDWe6j21I",
        "outputId": "fd58c4d4-03a1-4da2-cfaa-c305b30628bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchnet\n",
            "  Downloading torchnet-0.0.4.tar.gz (23 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchnet) (1.12.0+cu113)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchnet) (1.15.0)\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[K     |████████████████████████████████| 676 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchnet) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (1.7.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (2.23.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (23.2.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom->torchnet) (7.1.2)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torchnet) (3.0.4)\n",
            "Building wheels for collected packages: torchnet, visdom, torchfile\n",
            "  Building wheel for torchnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchnet: filename=torchnet-0.0.4-py3-none-any.whl size=29742 sha256=09bbb7169898ead4450048a87a2107fd9a9935b76c9065f28fe22a6fd84d7090\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/17/b3/86db1d93e9dae198813aa79831b403e4844d67986cf93894b5\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=93f1527b3d1d5c8876b5f281a8aceae6d74444e5b02e89b75a35fc0f4be67d57\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5709 sha256=641c1a0b0dd31015dde7064a1eed7e9f8860083d068d0d5962cfb456b5b413b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "Successfully built torchnet visdom torchfile\n",
            "Installing collected packages: jsonpointer, websocket-client, torchfile, jsonpatch, visdom, torchnet\n",
            "Successfully installed jsonpatch-1.32 jsonpointer-2.3 torchfile-0.1.0 torchnet-0.0.4 visdom-0.1.8.9 websocket-client-1.3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torchnet import meter\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "# from models import DeepVANetBio, DeepVANetVision, DeepVANet\n",
        "# from dataset import DEAP, MAHNOB, DEAPAll, MAHNOBAll\n",
        "# from utils import out_put"
      ],
      "metadata": {
        "id": "7uTlzr-ijdQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(modal, dataset, subject, k, l, epoch, lr, batch_size, file_name, indices, face_feature_size=16, bio_feature_size=64, use_gpu=False, pretrain=True):\n",
        "    '''\n",
        "    Train and test the model. Output the results.\n",
        "    :param modal: data modality\n",
        "    :param dataset: used dataset\n",
        "    :param subject: subject id\n",
        "    :param k: kth fold\n",
        "    :param l: emotional label\n",
        "    :param epoch: the number of epoches\n",
        "    :param lr: learn rate\n",
        "    :param batch_size: training batach size\n",
        "    :param file_name: result file name\n",
        "    :param indices: a list of index of the dataset\n",
        "    :param face_feature_size: face feature size\n",
        "    :param bio_feature_size: bio-sensing feature size\n",
        "    :param use_gpu: use gpu or not\n",
        "    :param pretrain: use pretrained cnn nor not\n",
        "    :return: the best test accuracy\n",
        "    '''\n",
        "    if use_gpu:\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    directory = file_name.split('/')[-2]\n",
        "    if not os.path.exists(f'./results/{dataset}/{modal}/'+directory):\n",
        "        os.mkdir(f'./results/{dataset}/{modal}/'+directory)\n",
        "\n",
        "    if dataset == 'DEAP':\n",
        "        ############## inter-subjects ##############\n",
        "        if subject == 0:\n",
        "            train_data = DEAPAll(modal=modal, k=k, kind='train', indices=indices, label=l)\n",
        "            val_data = DEAPAll(modal=modal, k=k, kind='val', indices=indices, label=l)\n",
        "        ############## per-subjects ##############\n",
        "        else:\n",
        "            train_data = DEAP(modal=modal,subject=subject,k=k,kind='train',indices=indices, label=l)\n",
        "            val_data = DEAP(modal=modal,subject=subject,k=k,kind='val',indices=indices, label=l)\n",
        "        bio_input_size = 40\n",
        "        peri_input_size = 8\n",
        "    if dataset == 'MAHNOB':\n",
        "        ############## inter-subjects  ##############\n",
        "        if subject == 0:\n",
        "            train_data = MAHNOBAll(modal=modal, k=k, kind='train', indices=indices, label=l)\n",
        "            val_data = MAHNOBAll(modal=modal, k=k, kind='val', indices=indices, label=l)\n",
        "        ############## per-subject #################\n",
        "        else:\n",
        "            train_data = MAHNOB(modal=modal,subject=subject,k=k,kind='train',indices=indices, label=l)\n",
        "            val_data = MAHNOB(modal=modal,subject=subject,k=k,kind='val',indices=indices, label=l)\n",
        "        bio_input_size = 38\n",
        "        peri_input_size = 6\n",
        "\n",
        "    # model\n",
        "    if modal == 'face':\n",
        "        model = DeepVANetVision(feature_size=face_feature_size,pretrain=pretrain).to(device)\n",
        "    if modal == 'bio':\n",
        "        model = DeepVANetBio(input_size=bio_input_size, feature_size=bio_feature_size).to(device)\n",
        "    if modal == 'eeg':\n",
        "        model = DeepVANetBio(input_size=32, feature_size=bio_feature_size).to(device)\n",
        "    if modal == 'peri':\n",
        "        model = DeepVANetBio(input_size=peri_input_size, feature_size=bio_feature_size).to(device)\n",
        "    if modal == 'faceeeg':\n",
        "        model = DeepVANet(bio_input_size=32, face_feature_size=face_feature_size, bio_feature_size=bio_feature_size, pretrain=pretrain).to(device)\n",
        "    if modal == 'faceperi':\n",
        "        model = DeepVANet(bio_input_size=peri_input_size, face_feature_size=face_feature_size, bio_feature_size=bio_feature_size, pretrain=pretrain).to(device)\n",
        "    if modal == 'facebio':\n",
        "        model = DeepVANet(bio_input_size=bio_input_size, face_feature_size=face_feature_size, bio_feature_size=bio_feature_size, pretrain=pretrain).to(device)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # criterion and optimizer\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    lr = lr\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
        "\n",
        "    # meters\n",
        "    loss_meter = meter.AverageValueMeter()\n",
        "\n",
        "    best_accuracy = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    # train\n",
        "    for epoch in range(epoch):\n",
        "        pred_label = []\n",
        "        true_label = []\n",
        "\n",
        "        loss_meter.reset()\n",
        "        for ii, (data,label) in enumerate(train_loader):\n",
        "            # print(ii)\n",
        "            # train model\n",
        "            if modal == 'faceeeg' or modal == 'faceperi' or modal == 'facebio':\n",
        "                input = (data[0].float().to(device), data[1].float().to(device))\n",
        "            else:\n",
        "                input = data.float().to(device)\n",
        "            label = label.float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(input).float()\n",
        "            # print(pred.shape,label.shape)\n",
        "            loss = criterion(pred, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # meters update\n",
        "            loss_meter.add(loss.item())\n",
        "\n",
        "            pred = (pred >= 0.5).float().to(device).data\n",
        "            pred_label.append(pred)\n",
        "            true_label.append(label)\n",
        "\n",
        "        pred_label = torch.cat(pred_label,0)\n",
        "        true_label = torch.cat(true_label,0)\n",
        "\n",
        "        train_accuracy = torch.sum(pred_label == true_label).type(torch.FloatTensor) / true_label.size(0)\n",
        "        out_put('Epoch: ' + 'train' + str(epoch) + '| train accuracy: ' + str(train_accuracy.item()), file_name)\n",
        "\n",
        "        val_accuracy = val(modal, model, val_loader, use_gpu)\n",
        "\n",
        "        out_put('Epoch: ' + 'train' + str(epoch) + '| train loss: ' + str(loss_meter.value()[0]) +\n",
        "              '| val accuracy: ' + str(val_accuracy.item()), file_name)\n",
        "\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            best_epoch = epoch\n",
        "            model.save(f\"{file_name}_best.pth\")\n",
        "\n",
        "    model.save(f'{file_name}.pth')\n",
        "\n",
        "    perf = f\"best accuracy is {best_accuracy} in epoch {best_epoch}\" + \"\\n\"\n",
        "    out_put(perf,file_name)\n",
        "\n",
        "    return best_accuracy"
      ],
      "metadata": {
        "id": "02ex7P_ikHBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def val(modal, model, dataloader, use_gpu):\n",
        "    model.eval()\n",
        "    if use_gpu:\n",
        "      device = torch.device('cuda')\n",
        "    else:\n",
        "      device = torch.device('cpu')\n",
        "\n",
        "    pred_label = []\n",
        "    true_label = []\n",
        "\n",
        "    for ii, (data, label) in enumerate(dataloader):\n",
        "        if modal == 'faceeeg' or modal == 'faceperi' or modal == 'facebio':\n",
        "            input = (data[0].float().to(device), data[1].float().to(device))\n",
        "        else:\n",
        "            input = data.float().to(device)\n",
        "        label = label.to(device)\n",
        "        pred = model(input).float()\n",
        "\n",
        "        pred = (pred >= 0.5).float().to(device).data\n",
        "        pred_label.append(pred)\n",
        "        true_label.append(label)\n",
        "\n",
        "    pred_label = torch.cat(pred_label, 0)\n",
        "    true_label = torch.cat(true_label, 0)\n",
        "\n",
        "    val_accuracy = torch.sum(pred_label == true_label).type(torch.FloatTensor) / true_label.size(0)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return "
      ],
      "metadata": {
        "id": "or2SOvY-kdnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision-level fusion functions"
      ],
      "metadata": {
        "id": "wlBEACNwkmbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "# from utils import out_put\n",
        "import os"
      ],
      "metadata": {
        "id": "0gd7VbeGkoGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adaboost(face_model_path, bio_model_path, bio_type, indices, dataset='DEAP', subject=1, k=1, label='valence', use_gpu=False):\n",
        "    if use_gpu:\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    # sub-classifiers\n",
        "    face_model = DeepVANetVision().to(device)\n",
        "    face_model.load(face_model_path)\n",
        "    if dataset == 'DEAP':\n",
        "        if bio_type == 'bio': bio_input_size = 40\n",
        "        if bio_type == 'eeg': bio_input_size = 32\n",
        "        if bio_type == 'peri': bio_input_size = 8\n",
        "    if dataset == 'MAHNOB':\n",
        "        if bio_type == 'bio': bio_input_size = 48\n",
        "        if bio_type == 'eeg': bio_input_size = 32\n",
        "        if bio_type == 'peri': bio_input_size = 6\n",
        "    bio_model = DeepVANetBio(bio_input_size).to(device)\n",
        "    bio_model.load(bio_model_path)\n",
        "\n",
        "    if dataset == 'DEAP':\n",
        "        face_train = DEAP(subject=subject, modal='face', label=label, kind='train', indices=indices, k=k)\n",
        "        face_test = DEAP(subject=subject, modal='face', label=label, kind='val', indices=indices, k=k)\n",
        "        bio_train = DEAP(subject=subject, modal=bio_type, label=label, kind='train', indices=indices, k=k)\n",
        "        bio_test = DEAP(subject=subject, modal=bio_type, label=label, kind='val', indices=indices, k=k)\n",
        "    if dataset == 'MAHNOB':\n",
        "        face_train = MAHNOB(subject=subject, modal='face', label=label, kind='train', indices=indices, k=k)\n",
        "        face_test = MAHNOB(subject=subject, modal='face', label=label, kind='val', indices=indices, k=k)\n",
        "        bio_train = MAHNOB(subject=subject, modal=bio_type, label=label, kind='train', indices=indices, k=k)\n",
        "        bio_test = MAHNOB(subject=subject, modal=bio_type, label=label, kind='val', indices=indices, k=k)\n",
        "\n",
        "    n_train = len(face_train)\n",
        "    n_test = len(face_test)\n",
        "\n",
        "    face_train_loader = DataLoader(face_train, batch_size=64, shuffle=False)\n",
        "    face_test_loader = DataLoader(face_test, batch_size=64, shuffle=False)\n",
        "    bio_train_loader = DataLoader(bio_train, batch_size=64, shuffle=False)\n",
        "    bio_test_loader = DataLoader(bio_test, batch_size=64, shuffle=False)\n",
        "\n",
        "    # initialize weights\n",
        "    w = np.ones(n_train) / n_train\n",
        "\n",
        "    face_pred = []\n",
        "    face_train_y = []\n",
        "    for ii, (x, y) in enumerate(face_train_loader):\n",
        "        print(ii)\n",
        "        pred = face_model(x.to(device)).cpu().detach().numpy()\n",
        "        face_pred.append(pred)\n",
        "        face_train_y.append(y.detach().numpy())\n",
        "    face_pred = np.concatenate(face_pred)\n",
        "    face_train_y = np.concatenate(face_train_y)\n",
        "    train_accuracy_face = sum((face_pred>=0.5).astype(float) == face_train_y) / n_train\n",
        "    I = (face_pred==face_train_y).astype(float)\n",
        "    I2 = np.array([1 if x==1 else -1 for x in I])\n",
        "    error = sum(abs(face_pred-face_train_y)*w)\n",
        "    alpha_face = 0.5 * np.log((1 - error)/error)\n",
        "    w_updated = np.multiply(w, np.exp([float(x) * alpha_face for x in I2]))\n",
        "\n",
        "    bio_pred = []\n",
        "    bio_train_y = []\n",
        "    for ii, (x, y) in enumerate(bio_train_loader):\n",
        "        print(ii)\n",
        "        pred = bio_model(x.to(device)).cpu().detach().numpy()\n",
        "        bio_pred.append(pred)\n",
        "        bio_train_y.append(y.detach().numpy())\n",
        "    bio_pred = np.concatenate(bio_pred)\n",
        "    bio_train_y = np.concatenate(bio_train_y)\n",
        "    train_accuracy_bio = sum((bio_pred >= 0.5).astype(float) == bio_train_y) / n_train\n",
        "    I = (bio_pred == bio_train_y).astype(float)\n",
        "    I2 = np.array([1 if x == 1 else -1 for x in I])\n",
        "    error = sum(abs(bio_pred - bio_train_y) * w_updated)\n",
        "    alpha_bio = 0.5 * np.log((1 - error)/error)\n",
        "\n",
        "    face_s = []\n",
        "    face_test_y = []\n",
        "    for ii, (x, y) in enumerate(face_test_loader):\n",
        "        print(ii)\n",
        "        pred = face_model(x.to(device)).cpu().detach().numpy()\n",
        "        face_s.append(pred)\n",
        "        face_test_y.append(y.detach().numpy())\n",
        "    face_s = np.concatenate(face_s)\n",
        "    face_test_y = np.concatenate(face_test_y)\n",
        "\n",
        "    bio_s = []\n",
        "    for ii, (x, y) in enumerate(bio_test_loader):\n",
        "        print(ii)\n",
        "        pred = bio_model(x.to(device)).cpu().detach().numpy()\n",
        "        bio_s.append(pred)\n",
        "    bio_s = np.concatenate(bio_s)\n",
        "\n",
        "    face_s_mapped = 2 * face_s - 1\n",
        "    bio_s_mapped = 2 * bio_s - 1\n",
        "\n",
        "    final_score = 1 / (1 + np.exp(-(alpha_face*face_s_mapped+alpha_bio*bio_s_mapped)))\n",
        "\n",
        "    final_pred = (final_score>=0.5).astype(float)\n",
        "    test_accuracy = sum(final_pred==face_test_y) / n_test\n",
        "\n",
        "    print(f'train accuracy face: {train_accuracy_face}, train accuracy bio: {train_accuracy_bio}, alpha_face: {alpha_face}, alpha_bio: {alpha_bio}, test accuracy: {test_accuracy}')\n",
        "    return train_accuracy_face, train_accuracy_bio, alpha_face, alpha_bio, test_accuracy\n"
      ],
      "metadata": {
        "id": "-cC8VKlgk2fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_fusion(dataset, modal, subject, k, label, indices, use_gpu, pretrain=True):\n",
        "    # train sub-classifiers\n",
        "    bio_modal = modal[4:]\n",
        "    if not os.path.exists(f'./results/'):\n",
        "        os.mkdir(f'./results/')\n",
        "    if not os.path.exists(f'./results/{dataset}/'):\n",
        "        os.mkdir(f'./results/{dataset}/')\n",
        "    if not os.path.exists(f'./results/{dataset}/face/'):\n",
        "        os.mkdir(f'./results/{dataset}/face/')\n",
        "    if not os.path.exists(f'./results/{dataset}/{bio_modal}/'):\n",
        "        os.mkdir(f'./results/{dataset}/{bio_modal}/')\n",
        "    train(modal='face', dataset=dataset, epoch=50, lr=0.001, use_gpu=use_gpu,\n",
        "               file_name=f'./results/{dataset}/face/{dataset}_decision_f_{label}_s{subject}_k{k}/{dataset}_decision_f_{label}_s{subject}_k{k}',\n",
        "               batch_size=64, subject=subject, k=k, l=label, indices=indices,pretrain=pretrain)\n",
        "    train(modal=bio_modal, dataset=dataset, epoch=50, lr=0.001, use_gpu=use_gpu,\n",
        "              file_name=f'./results/{dataset}/{bio_modal}/{dataset}_decision_{bio_modal}_{label}_s{subject}_k{k}/{dataset}_decision_{bio_modal}_{label}_s{subject}_k{k}',\n",
        "              batch_size=64, subject=subject, k=k, l=label, indices=indices)\n",
        "    face_model_path = f'./results/{dataset}/face/{dataset}_decision_f_{label}_s{subject}_k{k}/{dataset}_decision_f_{label}_s{subject}_k{k}.pth'\n",
        "    bio_model_path = f'./results/{dataset}/{bio_modal}/{dataset}_decision_{bio_modal}_{label}_s{subject}_k{k}/{dataset}_decision_{bio_modal}_{label}_s{subject}_k{k}.pth'\n",
        "\n",
        "    train_accuracy_face, train_accuracy_bio, alpha_face, alpha_bio, test_accuracy = adaboost(face_model_path, bio_model_path, bio_modal,indices, dataset, subject, k, label, use_gpu)\n",
        "    out_put(f'train accuracy face: {train_accuracy_face}, train accuracy bio: {train_accuracy_bio}, alpha_face: {alpha_face}, alpha_bio: {alpha_bio}, test accuracy: {test_accuracy}', f'./results/{dataset}/{modal}/{dataset}_decision_{label}_s{subject}_k{k}_{modal}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YiDo_bwElCQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo for per-subject experiment\n"
      ],
      "metadata": {
        "id": "2lguJeu9hGty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import train\n",
        "# from train import train\n",
        "# from decision_level_fusion import decision_fusion\n",
        "import random\n",
        "import argparse\n",
        "import os"
      ],
      "metadata": {
        "id": "-VK1YquTg9-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deap subject_id: sample number\n",
        "deap_indices_dict = {1: 2400,\n",
        "             2: 2400,\n",
        "             3: 2340,\n",
        "             4: 2400,\n",
        "             5: 2340,\n",
        "             6: 2400,\n",
        "             7: 2400,\n",
        "             8: 2400,\n",
        "             9: 2400,\n",
        "             10: 2400,\n",
        "             11: 2220,\n",
        "             12: 2400,\n",
        "             13: 2400,\n",
        "             14: 2340,\n",
        "             15: 2400,\n",
        "             16: 2400,\n",
        "             17: 2400,\n",
        "             18: 2400,\n",
        "             19: 2400,\n",
        "             20: 2400,\n",
        "             21: 2400,\n",
        "             22: 2400,\n",
        "             23: 2400,\n",
        "             24: 2400,\n",
        "             25: 2400,\n",
        "             26: 2400,\n",
        "             27: 2400,\n",
        "             28: 2400,\n",
        "             29: 2400,\n",
        "             30: 2400,\n",
        "             31: 2400,\n",
        "             32: 2400}"
      ],
      "metadata": {
        "id": "hmMU8Ricipwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo():\n",
        "    parser = argparse.ArgumentParser(description='Per-subject experiment')\n",
        "    parser.add_argument('--dataset', '-d', default='DEAP', help='The dataset used for evaluation', type=str)\n",
        "    parser.add_argument('--fusion', default='feature', help='Fusion strategy (feature or decision)', type=str)\n",
        "    parser.add_argument('--epoch', '-e', default=50, help='The number of epochs in training', type=int)\n",
        "    parser.add_argument('--batch_size', '-b', default=64, help='The batch size used in training', type=int)\n",
        "    parser.add_argument('--learn_rate', '-l', default=0.001, help='Learn rate in training', type=float)\n",
        "    parser.add_argument('--gpu', '-g', default='True', help='Use gpu or not', type=str)\n",
        "    # parser.add_argument('--file', '-f', default='./results/results.txt', help='File name to save the results', type=str)\n",
        "    parser.add_argument('--modal', '-m', default='facebio', help='Type of data to train', type=str)\n",
        "    parser.add_argument('--subject', '-s', default=1, help='Subject id', type=int)\n",
        "    parser.add_argument('--face_feature_size', default=16, help='Face feature size', type=int)\n",
        "    parser.add_argument('--bio_feature_size', default=64, help='Bio feature size', type=int)\n",
        "    parser.add_argument('--label', default='valence', help='Valence or arousal', type=str)\n",
        "    parser.add_argument('--pretrain',default='True', help='Use pretrained CNN', type=str)\n",
        "    parser.add_argument('-f')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    use_gpu = True if args.gpu == 'True' else False\n",
        "    pretrain = True if args.pretrain == 'True' else False\n",
        "\n",
        "    if args.dataset == 'DEAP':\n",
        "        indices = list(range(deap_indices_dict[args.subject]))\n",
        "    # if args.dataset == 'MAHNOB':\n",
        "    #     indices = list(range(mahnob_indices_dict[args.subject]))\n",
        "    # shuffle the dataset\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    if not os.path.exists(f'./results/'):\n",
        "        os.mkdir(f'./results/')\n",
        "    if not os.path.exists(f'./results/{args.dataset}/'):\n",
        "        os.mkdir(f'./results/{args.dataset}/')\n",
        "    if not os.path.exists(f'./results/{args.dataset}/{args.modal}/'):\n",
        "        os.mkdir(f'./results/{args.dataset}/{args.modal}/')\n",
        "\n",
        "    for k in range(1, 11):\n",
        "        if args.fusion == 'feature':\n",
        "            train(modal=args.modal, dataset=args.dataset, epoch=args.epoch, lr=args.learn_rate, use_gpu=use_gpu,\n",
        "                        file_name=f'./results/{args.dataset}/{args.modal}/{args.dataset}_{args.modal}_{args.label}_s{args.subject}_k{k}_{args.face_feature_size}_{args.bio_feature_size}/{args.dataset}_{args.modal}_{args.label}_s{args.subject}_k{k}_{args.face_feature_size}_{args.bio_feature_size}',\n",
        "                        batch_size=args.batch_size, subject=args.subject, k=k, l=args.label, indices=indices,\n",
        "                        face_feature_size=args.face_feature_size, bio_feature_size=args.bio_feature_size, pretrain=pretrain)\n",
        "        if args.fusion == 'decision':\n",
        "            decision_fusion(args.dataset, args.modal, args.subject, k, args.label, indices, use_gpu, pretrain)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lUR51E2qiwcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "myHfpvrNlQAg",
        "outputId": "459112ab-e4d5-48f3-d802-48a72089a0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-d6c23718ca70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-f6d31a3311fe>\u001b[0m in \u001b[0;36mdemo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'./results/{args.dataset}/{args.modal}/{args.dataset}_{args.modal}_{args.label}_s{args.subject}_k{k}_{args.face_feature_size}_{args.bio_feature_size}/{args.dataset}_{args.modal}_{args.label}_s{args.subject}_k{k}_{args.face_feature_size}_{args.bio_feature_size}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                         face_feature_size=args.face_feature_size, bio_feature_size=args.bio_feature_size, pretrain=pretrain)\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'decision'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mdecision_fusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-a05a4e51e9b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(modal, dataset, subject, k, l, epoch, lr, batch_size, file_name, indices, face_feature_size, bio_feature_size, use_gpu, pretrain)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m############## per-subjects ##############\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mbio_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-7fc7abd64096>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, modal, subject, k, kind, indices, label)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'./data/DEAP/faces/s{subject}.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'participant_ratings.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_zip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbio_zip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbio_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/DEAP/faces/s1.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "s4625_HNlbsG",
        "outputId": "ce731fe3-2846-4ff8-9b27-3eb6d47e5935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-d6c23718ca70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-2a6bea3f22c5>\u001b[0m in \u001b[0;36mdemo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--pretrain'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Use pretrained CNN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0muse_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'True'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1767\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1768\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2517\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2503\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2504\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 2"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szbf06A87kYM"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CSE499 Emotion Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNktzC7nUtK5k3jtQhkbYK8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}